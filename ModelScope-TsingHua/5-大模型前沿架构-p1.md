RAG框架



RAG应用



RAG问题



什么时候调用？

---



混合专家模型MoE

稀疏模型的回答效果优于普通模型。不同的token经过不同的router得到的output效果更好。[^ wenzhe]

主流：基于Transformer架构的稀疏模型

关键词：稀疏；稠密

稀疏=》解释：在某种条件下神经元被激活，从而部分参数参与计算。

稠密=》解释：几乎所有的参数都会对输入的数据进行处理的模型。

两者有何区分？[^ 算法朱丽叶]

​	训练：相同计算资源，MoE模型效果更好。

​	推理：

​				MoE：高显存，高吞吐量；

​				稠密模型：低显存，低吞吐量。

两篇论文带来的两个专家模型。

什么场景下使用？

特点有哪些？

​	预训练速度更快，推理速度更快。前者更快是因为能够并行训练每个专家模型，而普通的稠密模型需要串行地训练所有参数，导致训练速度慢。推理速度快是因为只需要激活与输入数据相关的专家模型，从而减少资源的使用。[^chatglm4]

挑战有哪些？

​	泛化能力不足，容易引起过拟合现象。

​	对显存的需求高。



当前的模型训练，如果需要什么模型，就需要重新训练一个模型得到一个新的模型。希望大语言模型能够实现模块化的过程。

重要前提：发现当前向传播的FFN层时，80%的神经元只有5%次被激活。这说明一次推理中，能够激活的神经元很少。希望它能够稀疏化地建模。





---



[^chatglm4]: https://chatglm.cn/main/gdetail/65d003612b6bf796463a3c2a?lang=zh
[^ 算法朱丽叶]: https://www.bilibili.com/video/BV1Ep421m7cx/?spm_id_from=333.337.search-card.all.click&vd_source=a8dc4471fd4266c4e803eba8f86ca23e
[^ wenzhe]: 【什么是混合专家模型（MoE)？】 https://www.bilibili.com/video/BV1vw4m1k78r/?share_source=copy_web&vd_source=28bdc0eeb51c36eeefed50a2bf58e928

