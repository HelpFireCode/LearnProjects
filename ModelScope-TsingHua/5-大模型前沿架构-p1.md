RAG框架



RAG应用



RAG问题



什么时候调用？

---



混合专家模型MoE——基于Transformer架构的稀疏模型

关键词：稀疏；稠密

稀疏=》解释：

稠密=》解释：



当前的模型训练，如果需要什么模型，就需要重新训练一个模型得到一个新的模型。希望大语言模型能够实现模块化的过程。

重要前提：发现当前向传播的FFN层时，80%的神经元只有5%次被激活。这说明一次推理中，能够激活的神经元很少。希望它能够稀疏化地建模。





